{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2C: ì›ë³¸ ì†ŒìŠ¤ ì¸ë±ì‹± ë…¸íŠ¸ë¶\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ìˆ˜ì§‘ëœ ì›ë³¸ ì†ŒìŠ¤ ë°ì´í„°ë¥¼ ì²­í‚¹í•˜ê³  ë²¡í„° DBì— ì¸ë±ì‹±í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ì›Œí¬í”Œë¡œìš°\n",
    "1. `source_collector.ipynb`ì—ì„œ ìƒì„±í•œ JSON íŒŒì¼ ë¡œë“œ\n",
    "2. ì†ŒìŠ¤ íƒ€ì…ë³„ë¡œ ì ì ˆí•œ ì²­í‚¹ ì „ëµ ì ìš©\n",
    "3. ê° ì²­í¬ì— ëŒ€í•´ 768ì°¨ì› ì„ë² ë”© ìƒì„±\n",
    "4. Chroma ë²¡í„° DBì— ì €ì¥ (ì¹´í…Œê³ ë¦¬ë³„ ë…ë¦½ ì»¬ë ‰ì…˜)\n",
    "\n",
    "## ë²¡í„° DB êµ¬ì¡°\n",
    "```\n",
    "./chroma_db/\n",
    "â”œâ”€â”€ fine_dining_sources/      # íŒŒì¸ë‹¤ì´ë‹ ì›ë³¸ ì†ŒìŠ¤\n",
    "â”œâ”€â”€ mid_price_sources/         # ì¤‘ì €ê°€ ì›ë³¸ ì†ŒìŠ¤\n",
    "â”œâ”€â”€ waiting_hotplace_sources/  # ì›¨ì´íŒ… í•«í”Œ ì›ë³¸ ì†ŒìŠ¤\n",
    "â”œâ”€â”€ fine_dining_examples/      # ê¸°ì¡´: ìš”ì•½ë¬¸ ì˜ˆì‹œ\n",
    "â”œâ”€â”€ mid_price_examples/\n",
    "â””â”€â”€ waiting_hotplace_examples/\n",
    "```\n",
    "\n",
    "## ë©”íƒ€ë°ì´í„° ìŠ¤í‚¤ë§ˆ\n",
    "ê° ì²­í¬ëŠ” ë‹¤ìŒ ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨:\n",
    "- `shop_seq`: ë§¤ì¥ ì‹œí€€ìŠ¤\n",
    "- `shop_name`: ë§¤ì¥ëª…\n",
    "- `category`: ì¹´í…Œê³ ë¦¬\n",
    "- `source_type`: ì†ŒìŠ¤ íƒ€ì…\n",
    "- `source_url`: ì¶œì²˜ URL\n",
    "- `chunk_index`: ì²­í¬ ì¸ë±ìŠ¤\n",
    "- `total_chunks`: ì „ì²´ ì²­í¬ ìˆ˜\n",
    "- `indexed_at`: ì¸ë±ì‹± ì‹œê°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì„¹ì…˜ 1: íŒ¨í‚¤ì§€ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-cloud-aiplatform google-genai python-dotenv chromadb langchain langchain-text-splitters pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì„¹ì…˜ 2: ë¼ì´ë¸ŒëŸ¬ë¦¬ Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vertexai\n",
    "from google import genai\n",
    "from dotenv import load_dotenv\n",
    "import chromadb\n",
    "from datetime import datetime\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import List, Dict\n",
    "import time\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ Import ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì„¹ì…˜ 3: Vertex AI ì´ˆê¸°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "PROJECT_ID = \"wad-dw\"\n",
    "LOCATION = \"us-central1\"\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "print(f\"âœ… Vertex AI ì´ˆê¸°í™” ì™„ë£Œ: {PROJECT_ID} ({LOCATION})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì„¹ì…˜ 4: Chroma ë²¡í„° DB ì´ˆê¸°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma í´ë¼ì´ì–¸íŠ¸ ìƒì„± (ê³µí†µ ë²¡í„° DB ì‚¬ìš©)\n",
    "# ê²½ë¡œ: knowledge_base/../chroma_db = shop_summary/chroma_db\n",
    "try:\n",
    "    chroma_client = chromadb.PersistentClient(path=\"../chroma_db\")\n",
    "    \n",
    "    # ì¹´í…Œê³ ë¦¬ë³„ ì†ŒìŠ¤ ì»¬ë ‰ì…˜ ìƒì„± (ê¸°ì¡´ example ì»¬ë ‰ì…˜ê³¼ ë¶„ë¦¬)\n",
    "    fine_dining_sources = chroma_client.get_or_create_collection(\n",
    "        name=\"fine_dining_sources\",\n",
    "        metadata={\"hnsw:space\": \"cosine\", \"description\": \"íŒŒì¸ë‹¤ì´ë‹ ì›ë³¸ ì†ŒìŠ¤\"}\n",
    "    )\n",
    "    \n",
    "    mid_price_sources = chroma_client.get_or_create_collection(\n",
    "        name=\"mid_price_sources\",\n",
    "        metadata={\"hnsw:space\": \"cosine\", \"description\": \"ì¤‘ì €ê°€ ì›ë³¸ ì†ŒìŠ¤\"}\n",
    "    )\n",
    "    \n",
    "    waiting_hotplace_sources = chroma_client.get_or_create_collection(\n",
    "        name=\"waiting_hotplace_sources\",\n",
    "        metadata={\"hnsw:space\": \"cosine\", \"description\": \"ì›¨ì´íŒ… í•«í”Œ ì›ë³¸ ì†ŒìŠ¤\"}\n",
    "    )\n",
    "    \n",
    "    # ì»¬ë ‰ì…˜ ë§¤í•‘\n",
    "    SOURCE_COLLECTIONS = {\n",
    "        \"fine_dining\": fine_dining_sources,\n",
    "        \"mid_price\": mid_price_sources,\n",
    "        \"waiting_hotplace\": waiting_hotplace_sources\n",
    "    }\n",
    "    \n",
    "    print(\"âœ… Chroma ë²¡í„° DB ì´ˆê¸°í™” ì™„ë£Œ (ê³µí†µ ë²¡í„° DB)\")\n",
    "    print(f\"   - ì €ì¥ ìœ„ì¹˜: ../chroma_db (shop_summary/chroma_db)\")\n",
    "    print(f\"\\n   âš ï¸ ìµœì´ˆ 1íšŒ ì‹¤í–‰ í›„, ëª¨ë“  ì¹´í…Œê³ ë¦¬ ë…¸íŠ¸ë¶ì—ì„œ ê³µìœ ë©ë‹ˆë‹¤!\")\n",
    "    print(f\"\\n   ì¹´í…Œê³ ë¦¬ë³„ ì»¬ë ‰ì…˜:\")\n",
    "    for category, collection in SOURCE_COLLECTIONS.items():\n",
    "        print(f\"   - {category}_sources: {collection.count()}ê°œ ì²­í¬\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Chroma ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\")\n",
    "    chroma_client = None\n",
    "    SOURCE_COLLECTIONS = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì„¹ì…˜ 5: ì²­í‚¹ ë° ì„ë² ë”© í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ì²­í‚¹ ì„¤ì • (source_chunking_experiment.ipynb ê²°ê³¼ ê¸°ë°˜)\n",
    "# ========================================\n",
    "\n",
    "CHUNKING_CONFIG = {\n",
    "    \"michelin_review\": {\n",
    "        \"method\": \"document\",\n",
    "        \"chunk_size\": None,\n",
    "        \"chunk_overlap\": 0\n",
    "    },\n",
    "    \"blueribbon_review\": {\n",
    "        \"method\": \"document\",\n",
    "        \"chunk_size\": None,\n",
    "        \"chunk_overlap\": 0\n",
    "    },\n",
    "    \"chef_interview\": {\n",
    "        \"method\": \"recursive\",\n",
    "        \"chunk_size\": 800,\n",
    "        \"chunk_overlap\": 200\n",
    "    },\n",
    "    \"course_description\": {\n",
    "        \"method\": \"semantic\",\n",
    "        \"chunk_size\": 512,\n",
    "        \"chunk_overlap\": 50\n",
    "    },\n",
    "    \"brand_philosophy\": {\n",
    "        \"method\": \"recursive\",\n",
    "        \"chunk_size\": 600,\n",
    "        \"chunk_overlap\": 100\n",
    "    },\n",
    "    \"space_ambiance\": {\n",
    "        \"method\": \"recursive\",\n",
    "        \"chunk_size\": 512,\n",
    "        \"chunk_overlap\": 50\n",
    "    },\n",
    "    \"tradition_technique\": {\n",
    "        \"method\": \"recursive\",\n",
    "        \"chunk_size\": 600,\n",
    "        \"chunk_overlap\": 100\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def chunk_text(text: str, source_type: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    ì†ŒìŠ¤ íƒ€ì…ì— ë§ëŠ” ì²­í‚¹ ì „ëµ ì ìš©\n",
    "    \"\"\"\n",
    "    config = CHUNKING_CONFIG.get(source_type, CHUNKING_CONFIG[\"brand_philosophy\"])\n",
    "    \n",
    "    if config[\"method\"] == \"document\":\n",
    "        # ë¶„í•  ì—†ìŒ\n",
    "        return [text]\n",
    "    \n",
    "    elif config[\"method\"] == \"recursive\":\n",
    "        # LangChain RecursiveCharacterTextSplitter\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=config[\"chunk_size\"],\n",
    "            chunk_overlap=config[\"chunk_overlap\"],\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \", \" \", \"\"],\n",
    "            length_function=len\n",
    "        )\n",
    "        return splitter.split_text(text)\n",
    "    \n",
    "    elif config[\"method\"] == \"semantic\":\n",
    "        # ê°„ë‹¨í•œ ë¬¸ì¥ ë‹¨ìœ„ ë¶„í• \n",
    "        sentences = text.replace(\". \", \".\\n\").replace(\"? \", \"?\\n\").replace(\"! \", \"!\\n\").split(\"\\n\")\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        chunk_size = config[\"chunk_size\"]\n",
    "        chunk_overlap = config[\"chunk_overlap\"]\n",
    "        \n",
    "        for sent in sentences:\n",
    "            sent = sent.strip()\n",
    "            if not sent:\n",
    "                continue\n",
    "            \n",
    "            if current_length + len(sent) > chunk_size and current_chunk:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "                \n",
    "                if chunk_overlap > 0 and current_chunk:\n",
    "                    overlap_text = current_chunk[-1]\n",
    "                    current_chunk = [overlap_text, sent]\n",
    "                    current_length = len(overlap_text) + len(sent)\n",
    "                else:\n",
    "                    current_chunk = [sent]\n",
    "                    current_length = len(sent)\n",
    "            else:\n",
    "                current_chunk.append(sent)\n",
    "                current_length += len(sent)\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    return [text]\n",
    "\n",
    "\n",
    "def generate_embedding(text: str, model=\"text-embedding-004\") -> List[float]:\n",
    "    \"\"\"í…ìŠ¤íŠ¸ë¥¼ 768ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜\"\"\"\n",
    "    try:\n",
    "        embedding_model = TextEmbeddingModel.from_pretrained(model)\n",
    "        embeddings = embedding_model.get_embeddings([text])\n",
    "        return embeddings[0].values\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def index_source_data(shop_data: Dict, collection):\n",
    "    \"\"\"\n",
    "    í•œ ë§¤ì¥ì˜ ì†ŒìŠ¤ ë°ì´í„°ë¥¼ ì²­í‚¹í•˜ê³  ë²¡í„° DBì— ì €ì¥\n",
    "    \n",
    "    Returns:\n",
    "        ì €ì¥ëœ ì²­í¬ ìˆ˜\n",
    "    \"\"\"\n",
    "    shop_seq = shop_data[\"shop_seq\"]\n",
    "    shop_name = shop_data[\"shop_name\"]\n",
    "    category = shop_data[\"category\"]\n",
    "    sources = shop_data[\"sources\"]\n",
    "    \n",
    "    total_chunks_stored = 0\n",
    "    \n",
    "    for source_idx, source in enumerate(sources):\n",
    "        source_type = source[\"source_type\"]\n",
    "        text = source[\"text\"]\n",
    "        url = source.get(\"url\", \"\")\n",
    "        \n",
    "        # ì²­í‚¹\n",
    "        chunks = chunk_text(text, source_type)\n",
    "        \n",
    "        # ê° ì²­í¬ë¥¼ ë²¡í„° DBì— ì €ì¥\n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            # ì„ë² ë”© ìƒì„±\n",
    "            embedding = generate_embedding(chunk)\n",
    "            \n",
    "            if embedding is None:\n",
    "                print(f\"   âš ï¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {shop_name} - {source_type} ì²­í¬ {chunk_idx}\")\n",
    "                continue\n",
    "            \n",
    "            # ê³ ìœ  ID ìƒì„±\n",
    "            doc_id = f\"{category}_{shop_seq}_{source_idx}_{chunk_idx}_{datetime.utcnow().strftime('%Y%m%d%H%M%S')}\"\n",
    "            \n",
    "            # Chromaì— ì €ì¥\n",
    "            collection.add(\n",
    "                documents=[chunk],\n",
    "                embeddings=[embedding],\n",
    "                metadatas=[{\n",
    "                    \"shop_seq\": shop_seq,\n",
    "                    \"shop_name\": shop_name,\n",
    "                    \"category\": category,\n",
    "                    \"source_type\": source_type,\n",
    "                    \"source_url\": url,\n",
    "                    \"chunk_index\": chunk_idx,\n",
    "                    \"total_chunks\": len(chunks),\n",
    "                    \"indexed_at\": datetime.utcnow().isoformat()\n",
    "                }],\n",
    "                ids=[doc_id]\n",
    "            )\n",
    "            \n",
    "            total_chunks_stored += 1\n",
    "            \n",
    "            # API Rate Limit ë°©ì§€\n",
    "            time.sleep(0.1)\n",
    "    \n",
    "    return total_chunks_stored\n",
    "\n",
    "\n",
    "print(\"âœ… ì²­í‚¹ ë° ì„ë² ë”© í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n",
    "print(\"   - chunk_text(): ì†ŒìŠ¤ íƒ€ì…ë³„ ì²­í‚¹\")\n",
    "print(\"   - generate_embedding(): 768ì°¨ì› ì„ë² ë”© ìƒì„±\")\n",
    "print(\"   - index_source_data(): ë²¡í„° DB ì¸ë±ì‹±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì„¹ì…˜ 6: ìˆ˜ì§‘ ë°ì´í„° ë¡œë“œ\n",
    "\n",
    "`source_collector.ipynb`ì—ì„œ ìƒì„±í•œ JSON íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# JSON íŒŒì¼ ë¡œë“œ\n",
    "# ========================================\n",
    "\n",
    "# JSON íŒŒì¼ ê²½ë¡œ ì…ë ¥\n",
    "JSON_FILE = \"collected_sources_20250111_120000.json\"  # ì‹¤ì œ íŒŒì¼ëª…ìœ¼ë¡œ ë³€ê²½í•˜ì„¸ìš”\n",
    "\n",
    "try:\n",
    "    with open(JSON_FILE, 'r', encoding='utf-8') as f:\n",
    "        collected_data = json.load(f)\n",
    "    \n",
    "    print(f\"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {JSON_FILE}\")\n",
    "    print(f\"   - ì´ ë§¤ì¥ ìˆ˜: {len(collected_data)}ê°œ\")\n",
    "    \n",
    "    # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„\n",
    "    category_counts = {}\n",
    "    for shop in collected_data:\n",
    "        cat = shop[\"category\"]\n",
    "        category_counts[cat] = category_counts.get(cat, 0) + 1\n",
    "    \n",
    "    print(\"\\n   ì¹´í…Œê³ ë¦¬ë³„:\")\n",
    "    for cat, count in category_counts.items():\n",
    "        print(f\"   - {cat}: {count}ê°œ\")\n",
    "    \n",
    "    # ì´ ì†ŒìŠ¤ ìˆ˜\n",
    "    total_sources = sum(len(shop[\"sources\"]) for shop in collected_data)\n",
    "    print(f\"\\n   - ì´ ì†ŒìŠ¤ ìˆ˜: {total_sources}ê°œ\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {JSON_FILE}\")\n",
    "    print(\"   source_collector.ipynbë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”.\")\n",
    "    collected_data = []\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}\")\n",
    "    collected_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì„¹ì…˜ 7: ë²¡í„° DB ì¸ë±ì‹± ì‹¤í–‰\n",
    "\n",
    "âš ï¸ ì£¼ì˜: ì´ ì„¹ì…˜ì€ Vertex AI APIë¥¼ í˜¸ì¶œí•˜ì—¬ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤. ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ë²¡í„° DB ì¸ë±ì‹±\n",
    "# ========================================\n",
    "\n",
    "if not collected_data:\n",
    "    print(\"âš ï¸ ì¸ë±ì‹±í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì„¹ì…˜ 6ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "else:\n",
    "    print(\"ğŸš€ ë²¡í„° DB ì¸ë±ì‹± ì‹œì‘\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nì˜ˆìƒ ì‹œê°„: ì•½ {len(collected_data) * 2}ì´ˆ (ë§¤ì¥ë‹¹ ì•½ 2ì´ˆ)\\n\")\n",
    "    \n",
    "    indexing_results = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for idx, shop_data in enumerate(collected_data, 1):\n",
    "        shop_name = shop_data[\"shop_name\"]\n",
    "        category = shop_data[\"category\"]\n",
    "        num_sources = len(shop_data[\"sources\"])\n",
    "        \n",
    "        print(f\"[{idx}/{len(collected_data)}] {shop_name} ({category})\")\n",
    "        print(f\"   ì†ŒìŠ¤ {num_sources}ê°œ ì²˜ë¦¬ ì¤‘...\")\n",
    "        \n",
    "        # ì¹´í…Œê³ ë¦¬ì— ë§ëŠ” ì»¬ë ‰ì…˜ ì„ íƒ\n",
    "        collection = SOURCE_COLLECTIONS.get(category)\n",
    "        \n",
    "        if not collection:\n",
    "            print(f\"   âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ì¹´í…Œê³ ë¦¬: {category}\")\n",
    "            indexing_results.append({\n",
    "                \"shop_name\": shop_name,\n",
    "                \"category\": category,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": \"Invalid category\"\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # ì¸ë±ì‹± ì‹¤í–‰\n",
    "            shop_start = time.time()\n",
    "            chunks_stored = index_source_data(shop_data, collection)\n",
    "            shop_elapsed = time.time() - shop_start\n",
    "            \n",
    "            print(f\"   âœ… ì™„ë£Œ: {chunks_stored}ê°œ ì²­í¬ ì €ì¥ ({shop_elapsed:.1f}ì´ˆ)\\n\")\n",
    "            \n",
    "            indexing_results.append({\n",
    "                \"shop_name\": shop_name,\n",
    "                \"category\": category,\n",
    "                \"status\": \"success\",\n",
    "                \"chunks_stored\": chunks_stored,\n",
    "                \"elapsed_time\": shop_elapsed\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ ì˜¤ë¥˜: {e}\\n\")\n",
    "            \n",
    "            indexing_results.append({\n",
    "                \"shop_name\": shop_name,\n",
    "                \"category\": category,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "    \n",
    "    total_elapsed = time.time() - start_time\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nâœ… ì¸ë±ì‹± ì™„ë£Œ (ì´ {total_elapsed:.1f}ì´ˆ)\\n\")\n",
    "    \n",
    "    # í†µê³„\n",
    "    success_count = sum(1 for r in indexing_results if r[\"status\"] == \"success\")\n",
    "    error_count = len(indexing_results) - success_count\n",
    "    total_chunks = sum(r.get(\"chunks_stored\", 0) for r in indexing_results)\n",
    "    \n",
    "    print(f\"ğŸ“Š í†µê³„:\")\n",
    "    print(f\"   - ì„±ê³µ: {success_count}ê°œ ë§¤ì¥\")\n",
    "    if error_count > 0:\n",
    "        print(f\"   - ì‹¤íŒ¨: {error_count}ê°œ ë§¤ì¥\")\n",
    "    print(f\"   - ì´ ì²­í¬: {total_chunks}ê°œ\")\n",
    "    print(f\"   - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {total_elapsed / len(collected_data):.1f}ì´ˆ/ë§¤ì¥\")\n",
    "    \n",
    "    # ì¹´í…Œê³ ë¦¬ë³„ ì»¬ë ‰ì…˜ í¬ê¸°\n",
    "    print(f\"\\n   ì¹´í…Œê³ ë¦¬ë³„ ì»¬ë ‰ì…˜ í¬ê¸°:\")\n",
    "    for category, collection in SOURCE_COLLECTIONS.items():\n",
    "        print(f\"   - {category}_sources: {collection.count()}ê°œ ì²­í¬\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì„¹ì…˜ 8: ê²°ê³¼ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ì¸ë±ì‹± ê²°ê³¼ ì €ì¥\n",
    "# ========================================\n",
    "\n",
    "if indexing_results:\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # CSV ì €ì¥\n",
    "    df_results = pd.DataFrame(indexing_results)\n",
    "    csv_filename = f\"indexing_results_{timestamp}.csv\"\n",
    "    df_results.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"ğŸ’¾ ê²°ê³¼ ì €ì¥: {csv_filename}\")\n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(df_results.to_string(index=False))\n",
    "else:\n",
    "    print(\"âš ï¸ ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì„¹ì…˜ 9: ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (ì„ íƒ)\n",
    "\n",
    "ì¸ë±ì‹±ëœ ë°ì´í„°ê°€ ì œëŒ€ë¡œ ê²€ìƒ‰ë˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
    "# ========================================\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬\n",
    "TEST_QUERY = \"ì…°í”„ì˜ ì¡°ë¦¬ ì² í•™ê³¼ ì œì²  ì‹ì¬ë£Œ í™œìš© ë°©ë²•\"\n",
    "\n",
    "print(f\"ğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\\n\")\n",
    "print(f\"ì¿¼ë¦¬: {TEST_QUERY}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±\n",
    "query_embedding = generate_embedding(TEST_QUERY)\n",
    "\n",
    "if query_embedding:\n",
    "    # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê²€ìƒ‰\n",
    "    for category, collection in SOURCE_COLLECTIONS.items():\n",
    "        if collection.count() == 0:\n",
    "            print(f\"\\nğŸ“‚ {category}: ì €ì¥ëœ ì²­í¬ ì—†ìŒ\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nğŸ“‚ {category}\")\n",
    "        print(\"â”€\" * 80)\n",
    "        \n",
    "        try:\n",
    "            results = collection.query(\n",
    "                query_embeddings=[query_embedding],\n",
    "                n_results=min(3, collection.count()),\n",
    "                include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "            )\n",
    "            \n",
    "            if results['ids'] and len(results['ids'][0]) > 0:\n",
    "                for i in range(len(results['ids'][0])):\n",
    "                    metadata = results['metadatas'][0][i]\n",
    "                    document = results['documents'][0][i]\n",
    "                    distance = results['distances'][0][i]\n",
    "                    similarity = 1.0 - (distance / 2.0)\n",
    "                    \n",
    "                    print(f\"\\n   [{i+1}] {metadata['shop_name']} - {metadata['source_type']}\")\n",
    "                    print(f\"      ìœ ì‚¬ë„: {similarity:.3f}\")\n",
    "                    print(f\"      ì²­í¬: {metadata['chunk_index'] + 1}/{metadata['total_chunks']}\")\n",
    "                    \n",
    "                    preview = document[:150] + \"...\" if len(document) > 150 else document\n",
    "                    print(f\"      ë‚´ìš©: {preview}\")\n",
    "            else:\n",
    "                print(\"   ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "else:\n",
    "    print(\"âŒ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "ì´ì œ ì›ë³¸ ì†ŒìŠ¤ê°€ ë²¡í„° DBì— ì¸ë±ì‹±ë˜ì—ˆìŠµë‹ˆë‹¤!\n",
    "\n",
    "### ë‹¤ìŒ ë…¸íŠ¸ë¶\n",
    "1. âœ… **source_chunking_experiment.ipynb** - ì²­í‚¹ ì „ëµ ì‹¤í—˜\n",
    "2. âœ… **source_collector.ipynb** - ì›ë³¸ ë°ì´í„° ìˆ˜ì§‘\n",
    "3. âœ… **source_indexer.ipynb** (í˜„ì¬) - ë²¡í„° DB ì¸ë±ì‹±\n",
    "4. â­ï¸ **main_rag_source.ipynb** - Source-based RAG (ì›ë³¸ ì†ŒìŠ¤ë§Œ ì‚¬ìš©)\n",
    "5. â­ï¸ **main_rag_hybrid.ipynb** - Hybrid RAG (ì›ë³¸ ì†ŒìŠ¤ + ìš”ì•½ë¬¸ ì˜ˆì‹œ)\n",
    "\n",
    "### í˜„ì¬ ë²¡í„° DB êµ¬ì¡°\n",
    "```\n",
    "./chroma_db/\n",
    "â”œâ”€â”€ fine_dining_sources/      â† ìƒˆë¡œ ì¶”ê°€ (ì›ë³¸ ì†ŒìŠ¤)\n",
    "â”œâ”€â”€ mid_price_sources/         â† ìƒˆë¡œ ì¶”ê°€\n",
    "â”œâ”€â”€ waiting_hotplace_sources/  â† ìƒˆë¡œ ì¶”ê°€\n",
    "â”œâ”€â”€ fine_dining_examples/      (ê¸°ì¡´: ìš”ì•½ë¬¸ ì˜ˆì‹œ)\n",
    "â”œâ”€â”€ mid_price_examples/\n",
    "â””â”€â”€ waiting_hotplace_examples/\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
