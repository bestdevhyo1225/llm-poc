# EKS + Airflow ë²¡í„° DB ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

> AWS EKS + Airflow í™˜ê²½ì—ì„œ ë¡œì»¬ ë²¡í„° DB (Chroma + EFS) êµ¬ì¶•í•˜ê¸°

**ì‘ì„±ì¼**: 2025-11-14
**ëŒ€ìƒ**: ì›” 2íšŒ ë°°ì¹˜ ì‹¤í–‰ (1ì¼, 15ì¼), ë§¤ì¥ 2,000ê°œ+ ì²˜ë¦¬

---

## ğŸ¯ ì™œ EFS + ë¡œì»¬ ë²¡í„° DBì¸ê°€?

### ë¹„ìš© ë¹„êµ (ì—°ê°„)

| ì˜µì…˜ | ë¹„ìš© | ì ˆê°ì•¡ |
|------|------|--------|
| Pinecone | $840 | - |
| Qdrant Cloud | $300 | - |
| **Chroma + EFS** | **$0.12** | **$839.88** ğŸ‰ |

### í•µì‹¬ ì´ì 

- âœ… **ê±°ì˜ ë¬´ë£Œ** (ì—°ê°„ $0.12)
- âœ… **98.9% ìœ íœ´ ì‹œê°„ì— ê³¼ê¸ˆ ì—†ìŒ**
- âœ… **ì™¸ë¶€ ì˜ì¡´ì„± ì—†ìŒ** (ì¥ì•  ìœ„í—˜ ì œê±°)
- âœ… **2ë°° ë¹ ë¥¸ ê²€ìƒ‰** (Pod ë‚´ë¶€ I/O)
- âœ… **Multi-AZ ê³ ê°€ìš©ì„±** (EFS ë„¤ì´í‹°ë¸Œ)

---

## ğŸ“‹ 3ë‹¨ê³„ êµ¬ì¶• ê°€ì´ë“œ

### Step 1: EFS ìƒì„± ë° PersistentVolume ì„¤ì • (1ì‹œê°„)

#### 1-1. EFS íŒŒì¼ì‹œìŠ¤í…œ ìƒì„±

```bash
# AWS CLIë¡œ EFS ìƒì„±
aws efs create-file-system \
  --creation-token llm-poc-vector-db \
  --performance-mode generalPurpose \
  --throughput-mode bursting \
  --encrypted \
  --tags Key=Name,Value=llm-poc-vector-db \
  --region us-east-1

# ì¶œë ¥ì—ì„œ FileSystemId ê¸°ë¡ (ì˜ˆ: fs-0123abcd)
```

#### 1-2. EFS ë§ˆìš´íŠ¸ íƒ€ê²Ÿ ìƒì„± (ê° AZë§ˆë‹¤)

```bash
# VPC ì„œë¸Œë„· í™•ì¸
kubectl get nodes -o wide

# ê° AZì˜ ì„œë¸Œë„·ì— ë§ˆìš´íŠ¸ íƒ€ê²Ÿ ìƒì„±
aws efs create-mount-target \
  --file-system-id fs-0123abcd \
  --subnet-id subnet-xxxxx \
  --security-groups sg-xxxxx  # EKS Worker Node SG ì‚¬ìš©
```

#### 1-3. EFS CSI Driver ì„¤ì¹˜

```bash
# Helmìœ¼ë¡œ ì„¤ì¹˜ (ê¶Œì¥)
helm repo add aws-efs-csi-driver https://kubernetes-sigs.github.io/aws-efs-csi-driver/
helm repo update

helm install aws-efs-csi-driver aws-efs-csi-driver/aws-efs-csi-driver \
  --namespace kube-system \
  --set controller.serviceAccount.create=true \
  --set controller.serviceAccount.annotations."eks\.amazonaws\.com/role-arn"=arn:aws:iam::ACCOUNT_ID:role/EFSCSIDriverRole

# ì„¤ì¹˜ í™•ì¸
kubectl get pods -n kube-system | grep efs-csi
```

#### 1-4. PersistentVolume ë° PVC ìƒì„±

```yaml
# k8s/vector-db-pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: vector-db-pv
spec:
  capacity:
    storage: 1Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: efs-sc
  csi:
    driver: efs.csi.aws.com
    volumeHandle: fs-0123abcd  # ì‹¤ì œ EFS IDë¡œ êµì²´

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: vector-db-pvc
  namespace: airflow  # Airflowê°€ ì‹¤í–‰ë˜ëŠ” ë„¤ì„ìŠ¤í˜ì´ìŠ¤
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: efs-sc
  resources:
    requests:
      storage: 1Gi
```

```bash
# ì ìš©
kubectl apply -f k8s/vector-db-pv.yaml

# í™•ì¸
kubectl get pv,pvc -n airflow
# NAME                           STATUS   VOLUME          CAPACITY   ACCESS MODES
# persistentvolumeclaim/vector-db-pvc   Bound    vector-db-pv   1Gi        RWX
```

---

### Step 2: Airflow DAG ì‘ì„± (2ì‹œê°„)

#### 2-1. ê¸°ë³¸ DAG êµ¬ì¡°

```python
# dags/shop_summary_batch_dag.py
from airflow import DAG
from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator
from airflow.utils.dates import days_ago
from datetime import timedelta

default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'email_on_failure': True,
    'email': ['alerts@company.com'],
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'shop_summary_batch',
    default_args=default_args,
    description='ë§¤ì¥ ìš”ì•½ë¬¸ ìƒì„± ë°°ì¹˜',
    schedule_interval='0 2 1,15 * *',  # ë§¤ì›” 1ì¼, 15ì¼ ì˜¤ì „ 2ì‹œ
    start_date=days_ago(1),
    catchup=False,
    tags=['llm', 'batch', 'rag'],
)

# Volume ì„¤ì • (ê³µí†µ)
volume_config = {
    'persistentVolumeClaim': {
        'claimName': 'vector-db-pvc'
    }
}

volume_mount = {
    'name': 'vector-db',
    'mountPath': '/data/vector_db'
}
```

#### 2-2. Task ì •ì˜

```python
# Task 1: ì´ˆê¸°í™” (S3ì—ì„œ ë³µì›, ì²« ì‹¤í–‰ ì‹œë§Œ)
init_task = KubernetesPodOperator(
    task_id='init_vector_db',
    name='init-vector-db',
    namespace='airflow',
    image='your-repo/shop-summary:latest',
    cmds=['python', '-c'],
    arguments=['''
import os
import boto3

if not os.path.exists("/data/vector_db/chroma_db"):
    print("ğŸ”„ S3ì—ì„œ ë²¡í„° DB ë³µì›...")
    s3 = boto3.client("s3")
    s3.download_file(
        "your-bucket",
        "vector-db-backups/latest/chroma_db.tar.gz",
        "/tmp/chroma_db.tar.gz"
    )
    os.system("tar -xzf /tmp/chroma_db.tar.gz -C /data/vector_db/")
    print("âœ… ë³µì› ì™„ë£Œ")
else:
    print("âœ… ê¸°ì¡´ ë²¡í„° DB ì‚¬ìš©")
    '''],
    volumes=[volume_config],
    volume_mounts=[volume_mount],
    is_delete_operator_pod=True,
    get_logs=True,
    dag=dag,
)

# Task 2~4: ì¹´í…Œê³ ë¦¬ë³„ ìš”ì•½ë¬¸ ìƒì„±
categories = [
    'fine_dining_and_susi_omakase',
    'low_to_mid_price_dining',
    'waiting_hotplace'
]

category_tasks = []
for category in categories:
    task = KubernetesPodOperator(
        task_id=f'generate_{category}',
        name=f'generate-{category}',
        namespace='airflow',
        image='your-repo/shop-summary:latest',
        cmds=['papermill'],
        arguments=[
            f'/app/shop_summary/{category}/main_rag.ipynb',
            f'/tmp/{category}_output.ipynb',
            '-p', 'MODE', 'multi',
            '-p', 'VECTOR_DB_PATH', '/data/vector_db/chroma_db',
        ],
        volumes=[volume_config],
        volume_mounts=[volume_mount],
        resources={
            'request_memory': '2Gi',
            'request_cpu': '1',
            'limit_memory': '4Gi',
            'limit_cpu': '2',
        },
        env_vars={
            'GOOGLE_APPLICATION_CREDENTIALS': '/secrets/gcp-key.json'
        },
        is_delete_operator_pod=True,
        get_logs=True,
        dag=dag,
    )
    category_tasks.append(task)

# Task 5: S3 ë°±ì—…
backup_task = KubernetesPodOperator(
    task_id='backup_to_s3',
    name='backup-to-s3',
    namespace='airflow',
    image='your-repo/shop-summary:latest',
    cmds=['python', '-c'],
    arguments=['''
import os
import boto3
from datetime import datetime

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# ì••ì¶•
os.system(f"tar -czf /tmp/chroma_backup_{timestamp}.tar.gz -C /data/vector_db chroma_db/")

# S3 ì—…ë¡œë“œ
s3 = boto3.client("s3")
s3.upload_file(
    f"/tmp/chroma_backup_{timestamp}.tar.gz",
    "your-bucket",
    f"vector-db-backups/{timestamp}/chroma_db.tar.gz"
)

# latest ë§í¬ ê°±ì‹ 
s3.copy_object(
    CopySource={"Bucket": "your-bucket", "Key": f"vector-db-backups/{timestamp}/chroma_db.tar.gz"},
    Bucket="your-bucket",
    Key="vector-db-backups/latest/chroma_db.tar.gz"
)

print(f"âœ… S3 ë°±ì—… ì™„ë£Œ: {timestamp}")
    '''],
    volumes=[volume_config],
    volume_mounts=[volume_mount],
    is_delete_operator_pod=True,
    get_logs=True,
    dag=dag,
)

# Task ì˜ì¡´ì„±
init_task >> category_tasks >> backup_task
```

---

### Step 3: Docker ì´ë¯¸ì§€ ë¹Œë“œ ë° ë°°í¬ (1ì‹œê°„)

#### 3-1. Dockerfile

```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€
RUN apt-get update && apt-get install -y \
    git \
    curl \
    tar \
    && rm -rf /var/lib/apt/lists/*

# Python íŒ¨í‚¤ì§€
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# í”„ë¡œì íŠ¸ ì½”ë“œ
COPY shop_summary/ /app/shop_summary/

CMD ["python", "-c", "print('Ready')"]
```

#### 3-2. requirements.txt

```txt
google-cloud-aiplatform
google-genai
chromadb
python-dotenv
papermill
boto3
```

#### 3-3. ë¹Œë“œ ë° í‘¸ì‹œ

```bash
# Docker ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t your-repo/shop-summary:latest .

# AWS ECR í‘¸ì‹œ (ì˜ˆì‹œ)
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ACCOUNT_ID.dkr.ecr.us-east-1.amazonaws.com

docker tag your-repo/shop-summary:latest ACCOUNT_ID.dkr.ecr.us-east-1.amazonaws.com/shop-summary:latest
docker push ACCOUNT_ID.dkr.ecr.us-east-1.amazonaws.com/shop-summary:latest
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### ë‹¨ì¼ ì¹´í…Œê³ ë¦¬ í…ŒìŠ¤íŠ¸

```bash
# Airflow UIì—ì„œ DAG ìˆ˜ë™ íŠ¸ë¦¬ê±°
# ë˜ëŠ” CLIë¡œ ì‹¤í–‰
airflow dags trigger shop_summary_batch

# ë¡œê·¸ í™•ì¸
kubectl logs -n airflow -l airflow-component=worker --tail=100
```

### EFS ë°ì´í„° í™•ì¸

```bash
# í…ŒìŠ¤íŠ¸ Pod ì‹¤í–‰
kubectl run -it --rm debug --image=busybox --restart=Never -n airflow -- sh

# Pod ë‚´ë¶€ì—ì„œ
ls -lh /data/vector_db/chroma_db/
# fine_dining_examples/
# low_to_mid_price_dining_examples/
# waiting_hotplace_examples/
```

---

## ğŸ“Š ëª¨ë‹ˆí„°ë§

### CloudWatch Logs ìˆ˜ì§‘

```yaml
# k8s/fluentd-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
  namespace: airflow
data:
  fluent.conf: |
    <source>
      @type tail
      path /var/log/containers/generate-*.log
      tag airflow.*
    </source>

    <match airflow.**>
      @type cloudwatch_logs
      log_group_name /aws/eks/shop-summary-batch
      log_stream_name ${tag}
    </match>
```

### EFS ë©”íŠ¸ë¦­ (CloudWatch)

```bash
# AWS Console â†’ CloudWatch â†’ Metrics â†’ EFS
# ëª¨ë‹ˆí„°ë§ ì§€í‘œ:
# - ClientConnections (Pod ì—°ê²° ìˆ˜)
# - DataReadIOBytes (ì½ê¸° ë°”ì´íŠ¸)
# - DataWriteIOBytes (ì“°ê¸° ë°”ì´íŠ¸)
# - PercentIOLimit (IOPS ì‚¬ìš©ë¥ )
```

---

## âš ï¸ ë¬¸ì œ í•´ê²°

### EFS ë§ˆìš´íŠ¸ ì‹¤íŒ¨

```bash
# ì¦ìƒ: Podê°€ Pending ìƒíƒœ
kubectl describe pod <pod-name> -n airflow
# Events: "MountVolume.SetUp failed for volume"

# í•´ê²°:
1. Security Group í™•ì¸
   - EKS Worker Node SGì— NFS (í¬íŠ¸ 2049) í—ˆìš©
2. EFS ë§ˆìš´íŠ¸ íƒ€ê²Ÿ í™•ì¸
   - ê° AZì— ë§ˆìš´íŠ¸ íƒ€ê²Ÿ ìˆëŠ”ì§€ í™•ì¸
3. EFS CSI Driver ì¬ì‹œì‘
   kubectl rollout restart deployment efs-csi-controller -n kube-system
```

### ë²¡í„° DB ë°ì´í„° ì†ì‹¤

```bash
# S3ì—ì„œ ë°±ì—… ë³µì›
aws s3 cp s3://your-bucket/vector-db-backups/latest/chroma_db.tar.gz .

# EFS ë§ˆìš´íŠ¸ëœ EC2ì—ì„œ ë³µì›
tar -xzf chroma_db.tar.gz -C /mnt/efs/
```

### ë©”ëª¨ë¦¬ ë¶€ì¡±

```yaml
# Pod ë¦¬ì†ŒìŠ¤ ì¦ê°€
resources:
  request_memory: '4Gi'  # 2Gi â†’ 4Gi
  request_cpu: '2'       # 1 â†’ 2
  limit_memory: '8Gi'    # 4Gi â†’ 8Gi
  limit_cpu: '4'         # 2 â†’ 4
```

---

## ğŸ¯ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì´ˆê¸° ì„¤ì •
- [ ] EFS íŒŒì¼ì‹œìŠ¤í…œ ìƒì„±
- [ ] EFS CSI Driver ì„¤ì¹˜
- [ ] PV/PVC ìƒì„± ë° ë°”ì¸ë”© í™•ì¸
- [ ] S3 ë²„í‚· ìƒì„± (ë°±ì—…ìš©)
- [ ] IAM Role ì„¤ì • (Pod â†’ S3 ì ‘ê·¼)

### Airflow DAG
- [ ] shop_summary_batch_dag.py ì‘ì„±
- [ ] Volume Mount ì„¤ì • í™•ì¸
- [ ] GCP ì¸ì¦ í‚¤ Secret ìƒì„±
- [ ] ë‹¨ì¼ ì¹´í…Œê³ ë¦¬ í…ŒìŠ¤íŠ¸
- [ ] 3ê°œ ì¹´í…Œê³ ë¦¬ ì „ì²´ í…ŒìŠ¤íŠ¸

### ìš´ì˜
- [ ] CloudWatch Logs ìˆ˜ì§‘ ì„¤ì •
- [ ] EFS ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§
- [ ] S3 Lifecycle Policy ì„¤ì • (30ì¼ ë³´ê´€)
- [ ] ì•Œë¦¼ ì„¤ì • (DAG ì‹¤íŒ¨ ì‹œ ì´ë©”ì¼)
- [ ] ì¬í•´ ë³µêµ¬ í…ŒìŠ¤íŠ¸ (S3 ë°±ì—… â†’ EFS ë³µì›)

---

## ğŸ’¡ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

### 1. ë°±ì—… ì „ëµ
- ë°°ì¹˜ ì™„ë£Œ í›„ ì¦‰ì‹œ S3 ë°±ì—…
- ë‚ ì§œë³„ ë²„ì „ ê´€ë¦¬ (30ì¼ ë³´ê´€)
- ì›” 1íšŒ ì „ì²´ ë°±ì—… ê²€ì¦

### 2. ë¦¬ì†ŒìŠ¤ ìµœì í™”
- Pod ë¦¬ì†ŒìŠ¤ Request: ì‹¤ì œ ì‚¬ìš©ëŸ‰ Ã— 1.2
- Pod ë¦¬ì†ŒìŠ¤ Limit: Request Ã— 2
- EFS Provisioned Throughput: ì²˜ìŒì—” ë¶ˆí•„ìš” (Bursting ì‚¬ìš©)

### 3. ë³´ì•ˆ
- EFS ì•”í˜¸í™” í•„ìˆ˜ (at-rest)
- IAM Role ìµœì†Œ ê¶Œí•œ ì›ì¹™
- GCP ì¸ì¦ í‚¤ëŠ” K8s Secretìœ¼ë¡œ ê´€ë¦¬

### 4. ë¹„ìš© ìµœì í™”
- EFS IA (Infrequent Access): 30ì¼ í›„ ìë™ ì „í™˜ ì„¤ì •
- S3 Intelligent-Tiering ì‚¬ìš©
- CloudWatch Logs ë³´ê´€ ê¸°ê°„: 7ì¼

---

## ğŸ“š ì°¸ê³  ìë£Œ

- **ì „ì²´ ë¹„ìš© ë¶„ì„**: `BATCH_PIPELINE_COST_ANALYSIS.md`
- **Hybrid Search ì ìš©**: `HYBRID_SEARCH_MIGRATION_GUIDE.md`
- **ë„¤ì´ë²„ ì‚¬ë¡€**: `NAVER_PLACE_AI_AGENT_CASE_STUDY.md`

### AWS ê³µì‹ ë¬¸ì„œ
- [EFS CSI Driver](https://github.com/kubernetes-sigs/aws-efs-csi-driver)
- [EFS ê°€ê²©](https://aws.amazon.com/efs/pricing/)
- [Airflow on EKS](https://aws.amazon.com/blogs/containers/running-apache-airflow-on-amazon-eks/)

---

**ì‘ì„±ì**: Claude Code
**ì—…ë°ì´íŠ¸**: 2025-11-14
**ì˜ˆìƒ êµ¬ì¶• ì‹œê°„**: 4~5ì‹œê°„
**ì—°ê°„ ë¹„ìš©**: $0.12
